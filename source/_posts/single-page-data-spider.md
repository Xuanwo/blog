title: 单页批量规则数据抓取解决方案
date: 2015-9-22 09:07:22
tags: [青云, Work, 爬虫]
categories: Opinion
toc: true
---
# 前言
加入青云后接的第一个活儿是抓取某公司的经销商的全部信息。该公司通过一个动态页面来展示经销商信息，使用后台的POST提交请求数据，并使用JS在页面下方加载请求的经销商信息，而且网址不会发生变化。

# 分析
接到活儿时候还在上课，电脑不在身边，就用手机先看了看页面。每一条数据都不出意外地非常有规律。观察辣个公司的页面源代码，看得头晕脑胀（只有一个萌萌的员工写的一句`别删我`让我傻乐了一会儿），无收获。没有办法使用BeautifulSoup，也不会用py来模拟用户的操作，我只好另辟蹊径了。

<!--more-->

# 方案

## 适用范围

- 单页
- 批量
- 规则数据（起码得比较有规则）

## 工具

- Chrome
- Sublime Text 3
- Clion
- Excel 2016

## 流程

- 使用Chrome访问指定页面，列出所有经销商
- `Ctrl+A`，然后`Ctrl+C`保存到ST3中（是的，你没看错）
- 使用ST3的查找替换功能处理部分规则文本替换为空格
- 使用Clion编写C++代码，将部分换行处理为空格
- 使用Excel的文本导入功能，使用空格作为分隔符，Done

# 坑点

## 编码

需要注意下输入和输出文本中均需使用GB2312(cp936)编码（为什么如此？）

## 不规范格式

辣个公司的部分经销商提供的信息不规范，主要有一下现象：
- 地址中存在空格（泥煤啊！）
- 区号和号码之间不使用`-`分隔

因为Excel会在导入文本时使用行作为行，使用每一个分隔符来区分列。所以这样的不规范信息会导致部分经销商的信息错误，需要手动修复一下。

# 总结
前期的调试和试错工作进行了大概两个小时，最后的实际工作只花了10分钟左右。最后处理了1500+的经销商信息，按照每个经销商需要操作30秒来计算，实际的效率大概提高了5.7倍，自我感觉还是很满意的。
不过这次经历也暴露出了我经验不足的缺点：在没有对自己的脚本进行充分测试之后就开始批量处理数据，结果手动处理数据时才发现这些特例数据的量太大，只能推倒重来，浪费了大量时间。

# 更新日志
- 2015年09月22日 完成初稿